{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pathlib\n",
    "import PIL.Image\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras import layers, models\n",
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from keras.layers import Flatten, SimpleRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and preprocessing of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_dir = 'D:/apps/apple_disease_dataset/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_set_dir = 'D:/apps/apple_disease_dataset/valid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_dir = 'D:/apps/apple_disease_dataset/test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "for dir_ in os.listdir(train_set_dir):\n",
    "    inner_dir = train_set_dir + \"/\" + dir_\n",
    "    for img in os.listdir(inner_dir):\n",
    "        img_path = inner_dir + \"/\" +img\n",
    "        img_arr = cv2.imread(img_path)\n",
    "        img_arr = cv2.resize(img_arr, (224, 224))\n",
    "        X_train.append(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = []\n",
    "for dir_ in os.listdir(valid_set_dir):\n",
    "    inner_dir = valid_set_dir + \"/\" + dir_\n",
    "    for img in os.listdir(inner_dir):\n",
    "        img_path = inner_dir + \"/\" +img\n",
    "        img_arr = cv2.imread(img_path)\n",
    "        img_arr = cv2.resize(img_arr, (224, 224))\n",
    "        X_valid.append(img_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7771\n",
      "1943\n"
     ]
    }
   ],
   "source": [
    "x_train = np.array(X_train)\n",
    "x_valid = np.array(X_valid)\n",
    "print(len(x_train))\n",
    "print(len(x_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise_data = ImageDataGenerator(rescale = 1./255)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7771 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "Y_train = normalise_data.flow_from_directory(train_set_dir, target_size = (224, 224), batch_size = 32, class_mode = 'sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Apple___Apple_scab': 0,\n",
       " 'Apple___Black_rot': 1,\n",
       " 'Apple___Cedar_apple_rust': 2,\n",
       " 'Apple___healthy': 3}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple___Apple_scab' 'Apple___Black_rot' 'Apple___Cedar_apple_rust'\n",
      " 'Apple___healthy']\n"
     ]
    }
   ],
   "source": [
    "classes = [\"Apple___Apple_scab\", \"Apple___Black_rot\", \"Apple___Cedar_apple_rust\", \"Apple___healthy\"]\n",
    "classes = np.array(classes)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1943 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "Y_valid = normalise_data.flow_from_directory(valid_set_dir, target_size = (224, 224), batch_size = 32, class_mode = 'sparse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = Y_train.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid = Y_valid.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = LabelEncoder()\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_valid = np_utils.to_categorical(lb.fit_transform(y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7771, 224, 224, 3)\n",
      "(7771, 4)\n",
      "(1943, 224, 224, 3)\n",
      "(1943, 4)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_valid.shape)\n",
    "print(y_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=x_train[0].shape)\n",
    "vgg_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 0\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential([\n",
    "    vgg_model,\n",
    "#     layers.Flatten(),\n",
    "#     layers.Dense(50, activation='relu'),\n",
    "#     layers.Dense(20, activation='relu'),\n",
    "#     layers.Dense(4, activation='softmax')\n",
    "    layers.TimeDistributed(Flatten()),\n",
    "    layers.SimpleRNN(64, return_sequences = False ),  \n",
    "    layers.Dense(20, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "#     layers.SimpleRNN(32, return_sequences = True),\n",
    "#     layers.SimpleRNN(4, return_sequences = False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Functional)           (None, 7, 7, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 7, 3584)           0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_14 (SimpleRNN)    (None, 64)                233536    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 20)                1300      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 84        \n",
      "=================================================================\n",
      "Total params: 14,949,608\n",
      "Trainable params: 234,920\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "243/243 [==============================] - 2736s 11s/step - loss: 0.6478 - accuracy: 0.7533 - val_loss: 0.4138 - val_accuracy: 0.8451\n",
      "Epoch 2/25\n",
      "243/243 [==============================] - 1872s 8s/step - loss: 0.4000 - accuracy: 0.8523 - val_loss: 0.3992 - val_accuracy: 0.8543\n",
      "Epoch 3/25\n",
      "243/243 [==============================] - 1713s 7s/step - loss: 0.3375 - accuracy: 0.8801 - val_loss: 0.4169 - val_accuracy: 0.8507\n",
      "Epoch 4/25\n",
      "243/243 [==============================] - 1743s 7s/step - loss: 0.3028 - accuracy: 0.8928 - val_loss: 0.3441 - val_accuracy: 0.8734\n",
      "Epoch 5/25\n",
      "243/243 [==============================] - 1732s 7s/step - loss: 0.2743 - accuracy: 0.9027 - val_loss: 0.3716 - val_accuracy: 0.8657\n",
      "Epoch 6/25\n",
      "243/243 [==============================] - 1656s 7s/step - loss: 0.2657 - accuracy: 0.9036 - val_loss: 0.3603 - val_accuracy: 0.8734\n",
      "Epoch 7/25\n",
      "243/243 [==============================] - 2130s 9s/step - loss: 0.2549 - accuracy: 0.9064 - val_loss: 0.3225 - val_accuracy: 0.8811\n",
      "Epoch 8/25\n",
      "243/243 [==============================] - 1965s 8s/step - loss: 0.2664 - accuracy: 0.9073 - val_loss: 0.3666 - val_accuracy: 0.8682\n",
      "Epoch 9/25\n",
      "243/243 [==============================] - 2032s 8s/step - loss: 0.2381 - accuracy: 0.9134 - val_loss: 0.3317 - val_accuracy: 0.8791\n",
      "Epoch 10/25\n",
      "243/243 [==============================] - 1789s 7s/step - loss: 0.2211 - accuracy: 0.9149 - val_loss: 0.3285 - val_accuracy: 0.8760\n",
      "Epoch 11/25\n",
      "243/243 [==============================] - 1690s 7s/step - loss: 0.2084 - accuracy: 0.9248 - val_loss: 0.3196 - val_accuracy: 0.8837\n",
      "Epoch 12/25\n",
      "243/243 [==============================] - 1864s 8s/step - loss: 0.2033 - accuracy: 0.9261 - val_loss: 0.3719 - val_accuracy: 0.8641\n",
      "Epoch 13/25\n",
      "243/243 [==============================] - 2380s 10s/step - loss: 0.2211 - accuracy: 0.9178 - val_loss: 0.3673 - val_accuracy: 0.8724\n",
      "Epoch 14/25\n",
      "243/243 [==============================] - 1834s 8s/step - loss: 0.2179 - accuracy: 0.9212 - val_loss: 0.3555 - val_accuracy: 0.8775\n",
      "Epoch 15/25\n",
      "243/243 [==============================] - 1719s 7s/step - loss: 0.1862 - accuracy: 0.9344 - val_loss: 0.3099 - val_accuracy: 0.8863\n",
      "Epoch 16/25\n",
      "243/243 [==============================] - 1715s 7s/step - loss: 0.1851 - accuracy: 0.9340 - val_loss: 0.2976 - val_accuracy: 0.8919\n",
      "Epoch 17/25\n",
      "243/243 [==============================] - 1818s 7s/step - loss: 0.1749 - accuracy: 0.9376 - val_loss: 0.3320 - val_accuracy: 0.8821\n",
      "Epoch 18/25\n",
      "243/243 [==============================] - 2337s 10s/step - loss: 0.1965 - accuracy: 0.9285 - val_loss: 0.3191 - val_accuracy: 0.8878\n",
      "Epoch 19/25\n",
      "243/243 [==============================] - 2017s 8s/step - loss: 0.1699 - accuracy: 0.9395 - val_loss: 0.3324 - val_accuracy: 0.8929\n",
      "Epoch 20/25\n",
      "243/243 [==============================] - 1705s 7s/step - loss: 0.1631 - accuracy: 0.9434 - val_loss: 0.3034 - val_accuracy: 0.9002\n",
      "Epoch 21/25\n",
      "243/243 [==============================] - 1776s 7s/step - loss: 0.1405 - accuracy: 0.9487 - val_loss: 0.3216 - val_accuracy: 0.8909\n",
      "Epoch 22/25\n",
      "243/243 [==============================] - 1744s 7s/step - loss: 0.1356 - accuracy: 0.9507 - val_loss: 0.3270 - val_accuracy: 0.8899\n",
      "Epoch 23/25\n",
      "243/243 [==============================] - 2005s 8s/step - loss: 0.1558 - accuracy: 0.9445 - val_loss: 0.3558 - val_accuracy: 0.8883\n",
      "Epoch 24/25\n",
      "176/243 [====================>.........] - ETA: 8:52 - loss: 0.1822 - accuracy: 0.9327"
     ]
    }
   ],
   "source": [
    "vgg_history = model.fit(x_train,y_train, epochs=25, validation_data=(x_valid,y_valid), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting accuracy vs validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vgg_history.history['accuracy'], label='train accuracy')\n",
    "plt.plot(vgg_history.history['val_accuracy'], label = 'validation accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting loss vs validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vgg_history.history['loss'], label='train loss')\n",
    "plt.plot(vgg_history.history['val_loss'], label = 'validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'VGG16_APPLE_LEAF_DISEASE_DETECTION_MODEL.h5'\n",
    "save_dir = os.path.join(os.getcwd(),'saved_models')\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print(\"Saved the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"saved_models/VGG16_APPLE_LEAF_DISEASE_DETECTION_MODEL.h5\")\n",
    "print(\"Loaded model\")\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics = ['accuracy'])\n",
    "score = loaded_model.evaluate(x_valid,y_valid, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting image 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path1 = test_set_dir + \"/\"+\"AppleCedarRust1.jpg\"\n",
    "img1 = plt.imread(img_path1)\n",
    "plt.imshow(img1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = image.load_img(img_path1, target_size=(224, 224))\n",
    "img_array1 = image.img_to_array(img1)\n",
    "img_batch1 = np.expand_dims(img_array1, axis =0)\n",
    "img_normalised1 = preprocess_input(img_batch1)\n",
    "pred1 = loaded_model.predict(img_normalised1)\n",
    "preds1 = pred1.argmax(axis = 1)\n",
    "prediction1 = preds1.astype(int).flatten()\n",
    "predictions1 = (lb.inverse_transform((prediction1)))\n",
    "print(classes[predictions1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting image 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path2 = test_set_dir + \"/\"+\"AppleCedarRust3.jpg\"\n",
    "img2 = plt.imread(img_path2)\n",
    "plt.imshow(img2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = image.load_img(img_path2, target_size=(224, 224))\n",
    "img_array2 = image.img_to_array(img2)\n",
    "img_batch2 = np.expand_dims(img_array2, axis =0)\n",
    "img_normalised2 = preprocess_input(img_batch2)\n",
    "pred2 = loaded_model.predict(img_normalised2)\n",
    "preds2 = pred2.argmax(axis = 1)\n",
    "prediction2 = preds2.astype(int).flatten()\n",
    "predictions2 = (lb.inverse_transform((prediction2)))\n",
    "print(classes[predictions2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting image 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path3 = test_set_dir + \"/\"+\"AppleScab1.jpg\"\n",
    "img3 = plt.imread(img_path3)\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img3 = image.load_img(img_path3, target_size=(224, 224))\n",
    "img_array3 = image.img_to_array(img3)\n",
    "img_batch3 = np.expand_dims(img_array3, axis =0)\n",
    "img_normalised3 = preprocess_input(img_batch3)\n",
    "pred3 = loaded_model.predict(img_normalised3)\n",
    "preds3 = pred3.argmax(axis = 1)\n",
    "prediction3 = preds3.astype(int).flatten()\n",
    "predictions3 = (lb.inverse_transform((prediction3)))\n",
    "print(classes[predictions3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
